import os
import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer

# --- 1. KONFIGURASI NAMA MODEL ---
# Kita pakai Gemma 2B versi standar HuggingFace (bukan unsloth)
base_model = "google/gemma-2b-it" 
new_model = "gemma-2b-dinar-chatbot"

# --- 2. LOAD DATASET (Format List of Lists) ---
# Pastikan file 'dataset_chatbot.json' sudah ada
import json

try:
    with open("dataset_chatbot.json", "r") as f:
        raw_data = json.load(f)
    
    # Mapping kolom sesuai struktur data kamu: [id, type, intent, instruction, response]
    df = pd.DataFrame(raw_data, columns=['id', 'type', 'intent', 'instruction', 'response'])
    
    # Konversi ke HuggingFace Dataset
    dataset = Dataset.from_pandas(df)
    print(f"‚úÖ Dataset berhasil dimuat: {len(dataset)} baris")

except Exception as e:
    print(f"‚ùå Error load dataset: {e}")
    # Dummy data agar kode tidak error saat copy-paste
    dataset = Dataset.from_dict({"instruction": ["Halo"], "response": ["Hai"]})

# --- 3. KONFIGURASI QLoRA (4-Bit Quantization) ---
# Ini penting agar hemat VRAM sesuai requirements bitsandbytes
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
)

# Load Model Dasar
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config,
    device_map="auto"
)
model.config.use_cache = False # Matikan cache saat training
model.config.pretraining_tp = 1

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# --- 4. SIAPKAN PEFT/LoRA ---
model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=8, # Rank matrix (bisa dinaikkan ke 16 jika VRAM cukup)
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

model = get_peft_model(model, peft_config)

# --- 5. FORMATTING PROMPT ---
def formatting_prompts_func(examples):
    output_texts = []
    for i in range(len(examples['instruction'])):
        instruction = examples['instruction'][i]
        response = examples['response'][i]
        # Format Chat Gemma
        text = f"<start_of_turn>user\n{instruction}<end_of_turn>\n<start_of_turn>model\n{response}<end_of_turn>"
        output_texts.append(text)
    return output_texts

# --- 6. TRAINING ARGUMENTS ---
training_arguments = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,          # 1 Epoch untuk 8300 data (cukup aman)
    per_device_train_batch_size=2, # Batch kecil agar aman
    gradient_accumulation_steps=4, # Akumulasi gradien
    optim="paged_adamw_32bit",
    save_steps=100,
    logging_steps=50,
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=True,
    bf16=False,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
)

# --- 7. JALANKAN TRAINER (TRL) ---
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    formatting_func=formatting_prompts_func, # TRL otomatis memproses ini
    max_seq_length=1024, # Batasi 1024 agar lebih ringan dari 2048
    tokenizer=tokenizer,
    args=training_arguments,
    packing=False,
)

print("üöÄ Mulai Training...")
trainer.train()

# Simpan Model
trainer.model.save_pretrained(new_model)
print("‚úÖ Training Selesai & Model Disimpan!")